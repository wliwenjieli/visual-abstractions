{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Computing Relational Bias\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load a pre-trained model using DeepNSD\n",
    "2. Extract features from relation task stimuli\n",
    "3. Compute distances between image embeddings\n",
    "4. Calculate the relational bias metric\n",
    "\n",
    "## About DeepJuice vs DeepNSD\n",
    "\n",
    "**DeepJuice** is currently in private beta and provides a streamlined API for model feature extraction. If you're interested in using DeepJuice, please contact the authors to request access.\n",
    "\n",
    "**DeepNSD** is the publicly available library that contains the core functionality of DeepJuice. For this demo, we use DeepNSD to ensure anyone can reproduce these results.\n",
    "\n",
    "**DeepNSD Public Repo:** https://github.com/ColinConwell/DeepNSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "The following cell will automatically check for DeepNSD and install it if needed. No manual setup required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì DeepNSD already installed\n",
      "‚úì Added DeepNSD to path: /user_data/wenjiel2/abstraction/submission_package/DeepNSD\n",
      "\n",
      "‚úì Setup complete! Ready to use DeepNSD for feature extraction.\n"
     ]
    }
   ],
   "source": [
    "# Setup DeepNSD Library\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "# Check if DeepNSD is already available\n",
    "deepnsd_path = Path(\"DeepNSD\")\n",
    "\n",
    "if not deepnsd_path.exists():\n",
    "    print(\"DeepNSD not found. Installing...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Clone the repository\n",
    "    print(\"\\n1. Cloning DeepNSD repository...\")\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"clone\", \"https://github.com/ColinConwell/DeepNSD.git\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(f\"‚ùå Error cloning repository: {result.stderr}\")\n",
    "        raise Exception(\"Failed to clone DeepNSD\")\n",
    "    \n",
    "    print(\"‚úì Repository cloned successfully\")\n",
    "    \n",
    "    # Install requirements\n",
    "    print(\"\\n2. Installing requirements...\")\n",
    "    requirements_file = deepnsd_path / \"requirements.txt\"\n",
    "    if requirements_file.exists():\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", str(requirements_file)],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            print(f\"‚ö† Warning: Some requirements may have failed to install\")\n",
    "            print(f\"Error: {result.stderr}\")\n",
    "        else:\n",
    "            print(\"‚úì Requirements installed successfully\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úì DeepNSD installation complete!\\n\")\n",
    "else:\n",
    "    print(\"‚úì DeepNSD already installed\")\n",
    "\n",
    "# Add DeepNSD to Python path\n",
    "if deepnsd_path.exists():\n",
    "    # Insert at position 0 to ensure it takes precedence\n",
    "    sys.path.insert(0, str(deepnsd_path.absolute()))\n",
    "    print(f\"‚úì Added DeepNSD to path: {deepnsd_path.absolute()}\")\n",
    "else:\n",
    "    print(\"‚ùå DeepNSD directory not found after installation\")\n",
    "    raise Exception(\"DeepNSD installation failed\")\n",
    "\n",
    "print(\"\\n‚úì Setup complete! Ready to use DeepNSD for feature extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Found DeepNSD model_opts: /user_data/wenjiel2/abstraction/submission_package/DeepNSD/source_code/pressures/model_opts\n",
      "‚úì Loaded model_options module\n",
      "‚úì Loaded feature_extraction module\n",
      "‚úì Added code directory to path: /user_data/wenjiel2/abstraction/submission_package/code/extraction\n",
      "‚úì Imports successful\n",
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: False\n",
      "Matplotlib backend: module://matplotlib_inline.backend_inline\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "import os\n",
    "import sys\n",
    "import importlib.util\n",
    "\n",
    "# Configure matplotlib for VS Code notebook display\n",
    "matplotlib.use('module://matplotlib_inline.backend_inline')\n",
    "\n",
    "# Load DeepNSD modules using importlib to handle relative imports\n",
    "deepnsd_model_opts = Path(os.getcwd()) / \"DeepNSD\" / \"source_code\" / \"pressures\" / \"model_opts\"\n",
    "if not deepnsd_model_opts.exists():\n",
    "    raise FileNotFoundError(f\"DeepNSD model_opts not found at {deepnsd_model_opts}\")\n",
    "\n",
    "print(f\"‚úì Found DeepNSD model_opts: {deepnsd_model_opts}\")\n",
    "\n",
    "# Step 1: Load model_options module first\n",
    "model_options_path = deepnsd_model_opts / \"model_options.py\"\n",
    "spec = importlib.util.spec_from_file_location(\"model_options\", model_options_path)\n",
    "model_options_module = importlib.util.module_from_spec(spec)\n",
    "sys.modules['model_options'] = model_options_module\n",
    "spec.loader.exec_module(model_options_module)\n",
    "get_model_options = model_options_module.get_model_options\n",
    "\n",
    "print(\"‚úì Loaded model_options module\")\n",
    "\n",
    "# Step 2: Load feature_extraction module\n",
    "# We need to inject the model_options contents to handle \"from .model_options import *\"\n",
    "feature_extraction_path = deepnsd_model_opts / \"feature_extraction.py\"\n",
    "\n",
    "# Read the source and replace the relative import\n",
    "with open(feature_extraction_path, 'r') as f:\n",
    "    source_code = f.read()\n",
    "\n",
    "# Replace relative import with absolute import\n",
    "source_code = source_code.replace('from .model_options import *', 'from model_options import *')\n",
    "\n",
    "# Create and execute the modified module\n",
    "spec = importlib.util.spec_from_file_location(\"feature_extraction\", feature_extraction_path)\n",
    "feature_extraction_module = importlib.util.module_from_spec(spec)\n",
    "sys.modules['feature_extraction'] = feature_extraction_module\n",
    "\n",
    "# Execute the modified code\n",
    "exec(compile(source_code, feature_extraction_path, 'exec'), feature_extraction_module.__dict__)\n",
    "get_all_feature_maps = feature_extraction_module.get_all_feature_maps\n",
    "\n",
    "print(\"‚úì Loaded feature_extraction module\")\n",
    "\n",
    "# Add code directory to path to import extraction functions\n",
    "code_dir = Path(os.getcwd()) / \"code\" / \"extraction\"\n",
    "if code_dir.exists():\n",
    "    sys.path.insert(0, str(code_dir))\n",
    "    print(f\"‚úì Added code directory to path: {code_dir}\")\n",
    "else:\n",
    "    print(f\"‚ö† Warning: Code directory not found at {code_dir}\")\n",
    "\n",
    "# Import extraction functions from code directory\n",
    "from core.model_processor import wrap_transform_with_resize\n",
    "from core.metrics import compute_triplet_distances\n",
    "\n",
    "print(\"‚úì Imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Matplotlib backend: {matplotlib.get_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Relation Task Data\n",
    "\n",
    "The relation task uses triplets of images:\n",
    "- **Sample**: The reference image\n",
    "- **Correct (Relational match)**: Shares the same abstract relation.\n",
    "- **Incorrect (Area match)**: Shares the same number of green pixels but different relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Working directory: /user_data/wenjiel2/abstraction/submission_package\n",
      "‚úì Data directory verified\n",
      "\n",
      "‚úì Loaded 126 trials\n",
      "\n",
      "First 5 trials:\n",
      "       Sample     Correct   Incorrect  sample_idx  correct_idx  incorrect_idx\n",
      "0  SST_45.png  SST_65.png  STS_45.png           9           47             10\n",
      "1  SST_45.png  SST_65.png  TST_45.png           9           47              0\n",
      "2  SST_45.png  SST_65.png  TTT_45.png           9           47             32\n",
      "3  SST_45.png  SST_85.png  SSS_45.png           9            4             28\n",
      "4  SST_45.png  SST_85.png  STS_45.png           9            4             10\n",
      "\n",
      "‚úì Found 50 stimulus images\n",
      "Example images: ['SSS_15.png', 'SSS_45.png', 'SST_35.png']\n"
     ]
    }
   ],
   "source": [
    "# Set paths relative to notebook location\n",
    "# This notebook should be run from the submission_package/ directory\n",
    "\n",
    "import os\n",
    "\n",
    "notebook_dir = Path(os.getcwd())\n",
    "\n",
    "STIMULI_DIR = notebook_dir / \"data/stimuli/relation\"\n",
    "PARQUET_FILE = notebook_dir / \"data/model_performance/relation.parquet\"\n",
    "\n",
    "print(f\"‚úì Working directory: {notebook_dir}\")\n",
    "print(f\"‚úì Data directory verified\")\n",
    "\n",
    "# Load trial information\n",
    "df = pd.read_parquet(PARQUET_FILE)\n",
    "\n",
    "# Extract key columns\n",
    "trial_info = df[['Sample', 'Correct', 'Incorrect', 'sample_idx', 'correct_idx', 'incorrect_idx']].copy()\n",
    "\n",
    "print(f\"\\n‚úì Loaded {len(trial_info)} trials\")\n",
    "print(f\"\\nFirst 5 trials:\")\n",
    "print(trial_info.head())\n",
    "\n",
    "# Get all unique image files\n",
    "all_images = sorted([str(f) for f in STIMULI_DIR.glob(\"*.png\")])\n",
    "print(f\"\\n‚úì Found {len(all_images)} stimulus images\")\n",
    "print(f\"Example images: {[Path(img).name for img in all_images[:3]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Example Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Figure saved: example_trial.png\n",
      "   View the image at: /user_data/wenjiel2/abstraction/submission_package/example_trial.png\n"
     ]
    }
   ],
   "source": [
    "# Visualize first trial\n",
    "trial_idx = 0\n",
    "sample_img = Image.open(STIMULI_DIR / trial_info.iloc[trial_idx]['Sample'])\n",
    "correct_img = Image.open(STIMULI_DIR / trial_info.iloc[trial_idx]['Correct'])\n",
    "incorrect_img = Image.open(STIMULI_DIR / trial_info.iloc[trial_idx]['Incorrect'])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].imshow(sample_img)\n",
    "axes[0].set_title(f\"Sample\\n{trial_info.iloc[trial_idx]['Sample']}\", fontsize=10)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(correct_img)\n",
    "axes[1].set_title(f\"Correct (Relational)\\n{trial_info.iloc[trial_idx]['Correct']}\", fontsize=10, color='green')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(incorrect_img)\n",
    "axes[2].set_title(f\"Incorrect (Perceptual)\\n{trial_info.iloc[trial_idx]['Incorrect']}\", fontsize=10, color='red')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "output_file = 'example_trial.png'\n",
    "fig.savefig(output_file, dpi=150, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "print(f\"‚úì Figure saved: {output_file}\")\n",
    "print(f\"   View the image at: {Path(output_file).absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model with DeepNSD\n",
    "\n",
    "DeepNSD provides access to models from multiple sources:\n",
    "- `torchvision_*` - TorchVision models\n",
    "- `timm_*` - TIMM library models  \n",
    "- `openclip_*` - OpenCLIP models\n",
    "\n",
    "Models are specified by their source, architecture, dataset, and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: resnet50_classification\n",
      "(Corresponds to CSV model: torchvision_resnet50_imagenet1k_v1)\n",
      "‚úì Model loaded successfully on cpu\n",
      "Model type: ResNet\n"
     ]
    }
   ],
   "source": [
    "# Select a model to test\n",
    "# \n",
    "# IMPORTANT: DeepNSD uses different naming than the stored results CSV\n",
    "# \n",
    "# DeepNSD format: {model_name}_{train_type}\n",
    "# CSV format: {source}_{model_name}_{dataset}_{version}\n",
    "# \n",
    "# Example mappings:\n",
    "# - DeepNSD: \"resnet50_classification\"  ‚Üí CSV: \"torchvision_resnet50_imagenet1k_v1\"\n",
    "# - DeepNSD: \"alexnet_classification\"   ‚Üí CSV: \"torchvision_alexnet_imagenet1k_v1\"\n",
    "# - DeepNSD: \"vgg16_classification\"     ‚Üí CSV: \"torchvision_vgg16_imagenet1k_v1\"\n",
    "#\n",
    "\n",
    "model_uid = \"resnet50_classification\"  # Corresponds to torchvision_resnet50_imagenet1k_v1\n",
    "\n",
    "# Map to CSV name for verification later\n",
    "model_uid_csv = \"torchvision_resnet50_imagenet1k_v1\"\n",
    "\n",
    "print(f\"Loading model: {model_uid}\")\n",
    "print(f\"(Corresponds to CSV model: {model_uid_csv})\")\n",
    "\n",
    "# Get model options and load model using DeepNSD\n",
    "model_options = get_model_options()\n",
    "if model_uid not in model_options:\n",
    "    print(f\"\\n‚ùå Error: Model '{model_uid}' not found in model options\")\n",
    "    print(f\"\\nAvailable models (first 10):\")\n",
    "    for i, uid in enumerate(list(model_options.keys())[:10]):\n",
    "        print(f\"  {i+1}. {uid}\")\n",
    "    raise ValueError(f\"Model {model_uid} not found in model options\")\n",
    "\n",
    "# Get the model loading function from model_options module\n",
    "# This makes the necessary functions available for eval()\n",
    "from model_options import get_torchvision_model, get_torchvision_transforms\n",
    "\n",
    "# Execute the model call to get model and preprocessing\n",
    "model_call = model_options[model_uid]['call']\n",
    "model = eval(model_call)\n",
    "\n",
    "# Get preprocessing transforms\n",
    "train_type = model_options[model_uid]['train_type']\n",
    "preprocess = get_torchvision_transforms(train_type, input_type='PIL')\n",
    "\n",
    "# Move to GPU if available and set to eval mode\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.eval()\n",
    "if device == 'cuda':\n",
    "    model = model.cuda()\n",
    "\n",
    "print(f\"‚úì Model loaded successfully on {device}\")\n",
    "print(f\"Model type: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Features from Stimuli\n",
    "\n",
    "We'll extract features from the penultimate layer (layer -2) of the model.\n",
    "\n",
    "**Important:** We wrap the model's preprocessing with a resize operation to ensure all images are consistently 224x224 before being processed. This matches the pipeline methodology used to generate the stored results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Wrapped preprocessing with resize to 224x224\n",
      "Created dataloader with batch size 16\n",
      "Total batches: 4\n",
      "Total images: 50\n"
     ]
    }
   ],
   "source": [
    "# Wrap preprocessing with resize to ensure consistent 224x224 images\n",
    "# This matches the pipeline preprocessing used to generate stored results\n",
    "wrapped_preprocess = wrap_transform_with_resize(preprocess, target_size=224)\n",
    "\n",
    "# Create dataset and dataloader manually\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "dataset = ImageDataset(all_images, wrapped_preprocess)\n",
    "batch_size = 16\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"‚úì Wrapped preprocessing with resize to 224x224\")\n",
    "print(f\"Created dataloader with batch size {batch_size}\")\n",
    "print(f\"Total batches: {len(dataloader)}\")\n",
    "print(f\"Total images: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from all layers...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdfee25bd4434d2e9e08d6aaa4ae1923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Feature Extraction (Batch):   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Feature extraction complete!\n",
      "Total layers extracted: 158\n",
      "\n",
      "Last 5 layers:\n",
      "  153: Conv2d-53 - shape (50, 100352)\n",
      "  154: BatchNorm2d-53 - shape (50, 100352)\n",
      "  155: ReLU-49 - shape (50, 100352)\n",
      "  156: AdaptiveAvgPool2d-1 - shape (50, 2048)\n",
      "  157: Linear-1 - shape (50, 1000)\n",
      "\n",
      "‚úì Selected penultimate layer: AdaptiveAvgPool2d-1\n",
      "Feature shape: (50, 2048)\n",
      "  - 50 images\n",
      "  - 2048 features per image\n"
     ]
    }
   ],
   "source": [
    "# Extract features using DeepNSD's get_all_feature_maps\n",
    "# This returns a dictionary of layer_name -> features\n",
    "\n",
    "print(f\"Extracting features from all layers...\")\n",
    "\n",
    "# Extract all feature maps\n",
    "feature_maps = get_all_feature_maps(\n",
    "    model=model,\n",
    "    inputs=dataloader,\n",
    "    remove_duplicates=True,  # Remove duplicate layers\n",
    "    flatten=True,            # Flatten spatial dimensions\n",
    "    numpy=True,              # Return as numpy arrays\n",
    "    use_tqdm=True            # Show progress bar\n",
    ")\n",
    "\n",
    "# Get layer names\n",
    "layer_names = list(feature_maps.keys())\n",
    "print(f\"\\n‚úì Feature extraction complete!\")\n",
    "print(f\"Total layers extracted: {len(layer_names)}\")\n",
    "print(f\"\\nLast 5 layers:\")\n",
    "for i, name in enumerate(layer_names[-5:]):\n",
    "    print(f\"  {len(layer_names) - 5 + i}: {name} - shape {feature_maps[name].shape}\")\n",
    "\n",
    "# Select the penultimate layer (layer -2)\n",
    "# This is the layer before the final classification layer\n",
    "penultimate_layer_name = layer_names[-2]\n",
    "features = feature_maps[penultimate_layer_name]\n",
    "\n",
    "print(f\"\\n‚úì Selected penultimate layer: {penultimate_layer_name}\")\n",
    "print(f\"Feature shape: {features.shape}\")\n",
    "print(f\"  - {features.shape[0]} images\")\n",
    "print(f\"  - {features.shape[1]} features per image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute Triplet Distances\n",
    "\n",
    "For each trial, we compute:\n",
    "- `dist_correct`: Cosine distance between sample and correct (relational) image\n",
    "- `dist_incorrect`: Cosine distance between sample and incorrect (perceptual) image\n",
    "- `distance_diff = dist_incorrect - dist_correct`\n",
    "\n",
    "**Interpretation:**\n",
    "- If `distance_diff > 0`: Model prefers relational match (correct)\n",
    "- If `distance_diff < 0`: Model prefers area match (incorrect)\n",
    "- If `distance_diff = 0`: Model is indifferent\n",
    "\n",
    "We use the `compute_triplet_distances()` function from `code/extraction/core/metrics.py` to ensure we match the exact pipeline methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created filename-based index mapping\n",
      "  Example: SST_45.png -> index 3\n",
      "\n",
      "‚úì Computed distances for 126 trials using pipeline function\n",
      "\n",
      "Distance difference statistics:\n",
      "  - Mean: -0.012868\n",
      "  - Std: 0.030074\n",
      "  - Min: -0.239551\n",
      "  - Max: 0.048380\n",
      "  - Median: -0.010973\n"
     ]
    }
   ],
   "source": [
    "# Create filename-based index mapping\n",
    "# Since we're using sorted images, we need to map filenames to their indices\n",
    "# in our sorted list (rather than using stored indices which were based on\n",
    "# a different glob order from the pipeline)\n",
    "\n",
    "import os\n",
    "\n",
    "# Create mapping from filename to index in our sorted image list\n",
    "filename_to_idx = {os.path.basename(img_path): idx \n",
    "                   for idx, img_path in enumerate(all_images)}\n",
    "\n",
    "# Map trial filenames to indices in our sorted image list\n",
    "sample_indices = trial_info['Sample'].apply(lambda x: filename_to_idx[x]).values\n",
    "correct_indices = trial_info['Correct'].apply(lambda x: filename_to_idx[x]).values\n",
    "incorrect_indices = trial_info['Incorrect'].apply(lambda x: filename_to_idx[x]).values\n",
    "\n",
    "print(f\"‚úì Created filename-based index mapping\")\n",
    "print(f\"  Example: {trial_info.iloc[0]['Sample']} -> index {sample_indices[0]}\")\n",
    "\n",
    "# Compute distances using imported function from code/extraction/core/metrics.py\n",
    "distance_diffs = compute_triplet_distances(\n",
    "    features,\n",
    "    sample_indices,\n",
    "    correct_indices,\n",
    "    incorrect_indices,\n",
    "    validate=False  # Skip validation for cleaner output\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Computed distances for {len(distance_diffs)} trials using pipeline function\")\n",
    "print(f\"\\nDistance difference statistics:\")\n",
    "print(f\"  - Mean: {distance_diffs.mean():.6f}\")\n",
    "print(f\"  - Std: {distance_diffs.std():.6f}\")\n",
    "print(f\"  - Min: {distance_diffs.min():.6f}\")\n",
    "print(f\"  - Max: {distance_diffs.max():.6f}\")\n",
    "print(f\"  - Median: {np.median(distance_diffs):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Distance Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Figure saved: distance_distribution.png\n",
      "   View the image at: /user_data/wenjiel2/abstraction/submission_package/distance_distribution.png\n",
      "\n",
      "Trials with positive difference (relational bias): 32 / 126\n",
      "Trials with negative difference (perceptual bias): 94 / 126\n",
      "Trials at zero: 0 / 126\n"
     ]
    }
   ],
   "source": [
    "# Plot distribution of distance differences\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.hist(distance_diffs, bins=30, edgecolor='black', alpha=0.7)\n",
    "ax.axvline(0, color='red', linestyle='--', linewidth=2, label='Chance (0)')\n",
    "ax.axvline(distance_diffs.mean(), color='blue', linestyle='--', linewidth=2, \n",
    "           label=f'Mean ({distance_diffs.mean():.4f})')\n",
    "\n",
    "ax.set_xlabel('Distance Difference (dist_incorrect - dist_correct)', fontsize=12)\n",
    "ax.set_ylabel('Number of Trials', fontsize=12)\n",
    "ax.set_title(f'Distribution of Distance Differences\\n{model_uid}', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "output_file = 'distance_distribution.png'\n",
    "fig.savefig(output_file, dpi=150, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "print(f\"‚úì Figure saved: {output_file}\")\n",
    "print(f\"   View the image at: {Path(output_file).absolute()}\")\n",
    "\n",
    "print(f\"\\nTrials with positive difference (relational bias): {(distance_diffs > 0).sum()} / {len(distance_diffs)}\")\n",
    "print(f\"Trials with negative difference (perceptual bias): {(distance_diffs < 0).sum()} / {len(distance_diffs)}\")\n",
    "print(f\"Trials at zero: {(distance_diffs == 0).sum()} / {len(distance_diffs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculate Relational Bias\n",
    "\n",
    "**Relational Bias** is the proportion of trials where the model prefers the relational match over the perceptual match.\n",
    "\n",
    "$$\\text{Relational Bias} = \\frac{\\text{# trials with } (\\text{dist_incorrect} - \\text{dist_correct}) > 0}{\\text{Total trials}}$$\n",
    "\n",
    "- **Relational Bias > 0.5**: Model shows relational bias (prefers abstract relations)\n",
    "- **Relational Bias < 0.5**: Model shows perceptual bias (prefers superficial features)\n",
    "- **Relational Bias = 0.5**: Model is at chance (no preference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RELATIONAL BIAS: 0.2540\n",
      "============================================================\n",
      "\n",
      "Interpretation: üî¥ AREA BIAS (significantly below chance)\n",
      "\n",
      "Model: resnet50_classification\n",
      "Total trials: 126\n",
      "Trials preferring relational match: 32\n",
      "Trials preferring area match: 94\n",
      "\n",
      "Note: Chance level is 0.5\n",
      "Binomial test threshold (126 trials, Œ±=0.05, one-sided):\n",
      "  - Relational bias: ‚â• 0.5794\n",
      "  - Area bias: ‚â§ 0.4206\n"
     ]
    }
   ],
   "source": [
    "def compute_relational_bias(distance_diffs):\n",
    "    \"\"\"\n",
    "    Compute relational bias as proportion of trials where model prefers\n",
    "    the relational match (distance_diff > 0).\n",
    "    \n",
    "    Args:\n",
    "        distance_diffs: Array of distance differences\n",
    "    \n",
    "    Returns:\n",
    "        Relational bias (proportion between 0 and 1)\n",
    "    \"\"\"\n",
    "    # Filter out any NaN values\n",
    "    valid_diffs = distance_diffs[~np.isnan(distance_diffs)]\n",
    "    \n",
    "    if len(valid_diffs) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    # Proportion where distance_diff > 0 (model prefers relational match)\n",
    "    relational_bias = (valid_diffs > 0).mean()\n",
    "    \n",
    "    return float(relational_bias)\n",
    "\n",
    "# Calculate relational bias\n",
    "relational_bias = compute_relational_bias(distance_diffs)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"RELATIONAL BIAS: {relational_bias:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Interpretation\n",
    "if relational_bias > 0.58:  # Binomial test threshold for 126 trials, Œ±=0.05\n",
    "    interpretation = \"üü¢ RELATIONAL BIAS (significantly above chance)\"\n",
    "elif relational_bias < 0.42:\n",
    "    interpretation = \"üî¥ AREA BIAS (significantly below chance)\"\n",
    "else:\n",
    "    interpretation = \"‚ö™ AT CHANCE (no clear preference)\"\n",
    "\n",
    "print(f\"\\nInterpretation: {interpretation}\")\n",
    "print(f\"\\nModel: {model_uid}\")\n",
    "print(f\"Total trials: {len(distance_diffs)}\")\n",
    "print(f\"Trials preferring relational match: {(distance_diffs > 0).sum()}\")\n",
    "print(f\"Trials preferring area match: {(distance_diffs < 0).sum()}\")\n",
    "print(f\"\\nNote: Chance level is 0.5\")\n",
    "print(f\"Binomial test threshold (126 trials, Œ±=0.05, one-sided):\")\n",
    "print(f\"  - Relational bias: ‚â• 0.5794\")\n",
    "print(f\"  - Area bias: ‚â§ 0.4206\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare with Human Performance\n",
    "\n",
    "The parquet file includes human behavioral data from different groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human and Monkey Performance:\n",
      "========================================\n",
      "(Averaged across all available trials)\n",
      "\n",
      "USADULT        : 0.9324 (n=74 trials)\n",
      "TSIADULT       : 0.6633 (n=15 trials)\n",
      "KID            : 0.8150 (n=30 trials)\n",
      "MONKEY         : 0.4128 (n=20 trials)\n",
      "\n",
      "Model Performance:\n",
      "========================================\n",
      "resnet50_classification: 0.2540 (n=126 trials)\n"
     ]
    }
   ],
   "source": [
    "# Extract human performance (averaged across all trials)\n",
    "human_cols = ['USADULT_Accuracy', 'TSIADULT_Accuracy', 'KID_Accuracy', 'MONKEY_Accuracy']\n",
    "\n",
    "print(\"Human and Monkey Performance:\")\n",
    "print(\"=\"*40)\n",
    "print(\"(Averaged across all available trials)\\n\")\n",
    "\n",
    "available_data = False\n",
    "for col in human_cols:\n",
    "    if col in df.columns:\n",
    "        values = df[col].dropna()  # Remove NaN values\n",
    "        if len(values) > 0:\n",
    "            group = col.replace('_Accuracy', '')\n",
    "            mean_accuracy = values.mean()\n",
    "            n_trials = len(values)\n",
    "            print(f\"{group:15s}: {mean_accuracy:.4f} (n={n_trials} trials)\")\n",
    "            available_data = True\n",
    "\n",
    "if not available_data:\n",
    "    print(\"(No human data available)\")\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "print(\"=\"*40)\n",
    "print(f\"{model_uid:15s}: {relational_bias:.4f} (n={len(distance_diffs)} trials)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepnsd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
